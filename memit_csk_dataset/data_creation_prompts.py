import argparse
import re
from typing import List, Dict, Any

from gptinference import utils
from gptinference.base_prompt import Prompt
from gptinference.openai_wrapper import OpenAIWrapper
from tqdm import tqdm

"""
    A script that uses zero shot prompting to: 
    (i) Naturalize 20Q and PEP3K dataset.
    For example: input c = whale eat plankton
    
    Rephrase to make grammatical:
    output1 = whales eat plankton
    
    Negate the sentence:
    output2 = whales do not eat plankton

    (ii) Prepare neighborhood evaluation dataset to ensure relevant neighborhood is affected 
         and irrelevant facts are unaffected as a result of model editing.

    Example of neighborhood
    ---------
     "affected_paraphrase": [
        "People can be bitten by wasps.",
        "Wasps can puncture human skin with their sting.",
        "Wasps can deliver a painful bite to people.",
        "Wasps can cause a person to experience a bite.",
        "Wasps can inflict a sting on humans."
      ]
     "affected_reasoning": [
        "Wasps are predators that hunt for food",
        "Humans can provide food source for wasps."
      ]
     "affected_neighborhood": 
         "subject_replaced": [
            "Bees sting humans",
            "Yellowjackets sting humans",
            "Hornets sting humans",
            "Fire ants sting humans",
            "Mosquitoes sting humans."
          ],
          "verb_replaced": [
            "Wasps attacked humans",
            "Wasps stung humans",
            "Wasps harassed humans",
            "Wasps provoked humans",
            "Wasps irritated humans."
          ],
          "object_replaced": [
            "Wasps bite people",
            "Wasps bite victims.",
            "Wasps bite victims"
          ]
    "Unaffected_neighborhood":
          "subject_replaced": [
            "Dogs bite humans",
            "Monkeys bite humans",
            "Fish bite humans",
            "Birds bite humans",
            "Cats bite humans"
          ],
          "object_replaced": [
            "Wasps bite fish",
            "Wasps bite animals",
            "Wasps bite insects",
            "Wasps bite birds",
            "Wasps bite reptiles"
          ]
     
    ...
    
    Classes
    ----------
    Naturalizing and negating facts (TFGrammatiker* and TFGrammatikerNeg*)
    Affected (Neighborhood, Paraphrase, Reasoning)
    Unaffected (Neighborhood)
    
"""


def parse_enumerated(s: str) -> str:
    # Parse an item from an enumeration generated by GPT.
    # 1. Apples are red ==> Apples are red
    # 1: Apples are red ==> Apples are red
    # First: Apples are red ==> Apples are red
    return re.sub(r'\d\.', '', s.strip()).split(":")[-1].split(". ")[-1].strip()


# Given: 1.abc\n2.def ==> [abc, def]
def parse_contracted_list_of_spo(q1s):
    q1s_new = []
    if len(q1s) < 3:
        for q1 in q1s:
            for q in q1.split("\n" if "\n" in q1 else (";" if ";" in q1 else ",")):
                q1s_new.append(q)
    else:
        for q1 in q1s:
            q1s_new.append(q1)
    return [x.replace("Q ", "") for x in q1s_new if len(x) > 5]


def compress_obj_text(x):
    # reduce o in svo to 1 word
    return x.strip().replace(" the ", " ").replace(" a ", " ").replace(" an ", " ")


class TFGrammatikerPos(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, s: str) -> str:
        return f"Given the text: {s}.\nFix the grammar and write the grammatical sentences.\n"

    def __call__(self, s: str, subj: str = None, obj: str = None, verb: str = None) -> Dict[str, Any]:
        generation_query = self.make_query(s=s)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=20, stop_token="###", temperature=0.0)
        return {"TFGrammatikerPos": generated_sent,
                 "grammatical": generated_sent.strip()
                }


class TFGrammatikerPosBatched(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, triples_csv: str) -> str:
        question_prefix_template = f"""You are given some input sentences. Fix the grammar and write the grammatical sentences.

inputs: {triples_csv}

outputs:
"""
        query = f"""{self.question_prefix}{question_prefix_template}"""
        return query

    def __call__(self, triples_csv: str) -> Dict[str, Any]:
        generation_query = self.make_query(triples_csv=triples_csv)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=300, stop_token="###", temperature=0.0)
        return {"TFGrammatikerPosBatched": generated_sent,
                 "grammaticalBatched": [compress_obj_text(x) for x in generated_sent.split(",")]
                }


class TFGrammatikerNegBatched(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, triples_csv: str) -> str:
        question_prefix_template = f"""You are given some input sentences. Fix the grammar and write the negated sentence by replacing the verb.
    
inputs: {triples_csv}
    
outputs:
"""
        query = f"""{self.question_prefix}{question_prefix_template}"""
        return query

    def __call__(self, triples_csv: str) -> Dict[str, Any]:
        generation_query = self.make_query(triples_csv=triples_csv)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=300, stop_token="###", temperature=0.0)
        return {
            "TFGrammatikerNegBatched": generated_sent,
            "negatedBatched": [compress_obj_text(x) for x in generated_sent.split(",")]
        }


class TFGrammatikerNeg(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, s: str) -> str:
        return f"Given the text: {s}.\nWrite the negated sentence by replacing the verb.\n"

    def __call__(self, s: str, subj: str= None, obj: str= None, verb: str= None) -> Dict[str, Any]:
        generation_query = self.make_query(s=s)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=20, stop_token="###", temperature=0.0)
        return {
            "UnAffectedNeighborhoodMaker_gptinput": generation_query,
            "UnAffectedNeighborhoodMaker_gptoutput": generated_sent,
            "negated": generated_sent.strip()
        }


class AffectedParaphraseMaker(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, s: str) -> str:
        return f"Provide 5 paraphrases of: {s}\n"

    def __call__(self, s: str, subj: str= None, obj: str= None, verb: str= None) -> Dict[str, Any]:
        generation_query = self.make_query(s=s)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=100, stop_token="###", temperature=0.0)
        return {
            "AffectedParaphraseMaker_gptinput": generation_query,
            "AffectedParaphraseMaker_gptoutput": generated_sent,
            "affected_paraphrase": dedup([parse_enumerated(x) for x in generated_sent.split("\n") if len(x) > 10])
        }



class AffectedNeighborhoodMaker(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, s: str, subj: str, obj: str, verb: str) -> str:
        # "[X] eat plankton": subject [X] can be a subtype of [.] e.g., Minke whales eat plankton
        # "Whales [X] plankton": verb [X] can be an event that follows or precedes [.] e.g., Whales hunt plankton, Whales look for plankton
        # "Whales eat [X]": object [X] can be subtypes or similar neighbors of [.], e.g., Whales eat Phytoplankton, Zooplankton
        return f"Given the text: {s}\nsubject token: {subj}\nobject token: {obj}\n" \
               f"Q1. In the text, replace just the subject token with a different word. The replaced text should be a valid sentence. The replaced token can be a hyponym or similar word of the original subject token. Write up to 5 such variants.\n" \
               f"Q2. In the text, replace just the verb token with a different word. The replaced text should be a valid sentence. The replaced token can be a verb that follows or precedes the original verb token. Write up to 5 such variants.\n" \
               f"Q3. In the text, replace just the object token with a different word. The replaced text should be a valid sentence. The replaced token can be a hyponym or similar word of the original object token. Write up to 5 such variants.\n"\


    def __call__(self, s: str, subj: str, obj: str, verb: str) -> Dict[str, Any]:
        generation_query = self.make_query(s=s, subj=subj, obj=obj, verb=verb)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=300, stop_token="###", temperature=0.0)
        q1, q2 = generated_sent.strip().split("\nQ2.")
        q2, q3 = q2.split("\nQ3.")
        split_enumeration_on = "\n" if "\n" in q2.strip() else ","  # GPT can return a list as a csv or as enumeration
        q1s = dedup([parse_enumerated(x) for x in q1.strip().replace("Q1. ", "").split(split_enumeration_on) if len(x) > 10])
        q2s = dedup([parse_enumerated(x) for x in q2.strip().replace("Q2. ", "").split(split_enumeration_on) if len(x) > 10])
        q3s = dedup([parse_enumerated(x) for x in q3.strip().replace("Q3. ", "").split(split_enumeration_on) if len(x) > 10])

        q1s = parse_contracted_list_of_spo(q1s)
        q2s = parse_contracted_list_of_spo(q2s)
        q3s = parse_contracted_list_of_spo(q3s)

        return {
                "AffectedNeighborhoodMaker_gptinput": generation_query,
                "AffectedNeighborhoodMaker_gptoutput": generated_sent,
                "subject_replaced": q1s,
                "verb_replaced": q2s,
                "object_replaced": q3s
        }


def dedup(lst):
    return list(set(lst))


class UnAffectedNeighborhoodMaker(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, s: str, subj: str, obj: str) -> str:
        # "[X] eat plankton": subject [X] is different from [.] e.g., Snails eat plankton
        # "Whales eat [X]": object [X] is different from [.], e.g., Whales eat seaweed.
        # return f"Given the text: {s}\nsubject token: {subj}\nobject token: {obj}\nQ1. Replace the subject token with an unrelated word to make a new text.\nQ2. Replace the object token with an unrelated word to make a new text.\n"
        return f"""Given:
text: {s}
subject token: {subj}
object token: {obj}

Q1. Replace the subject token with a completely unrelated word and make a new text. Make 5 such replacements.
Q2. Replace the object token with a completely unrelated word and make a new text. Make 5 such replacements.


"""

    def __call__(self, s: str, subj: str, obj: str, verb: str= None) -> Dict[str, Any]:
        generation_query = self.make_query(s=s, subj=subj, obj=obj)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=300, stop_token="###", temperature=0.0)
        split_on_1 = "Replacements for the object token:"
        split_on_2 = "Replacing the object token:"
        split_on_3 = "\n\n"
        split_on = "\nQ2." if "\nQ2." in generated_sent else (split_on_1 if split_on_1 in generated_sent else (split_on_2 if split_on_2 in generated_sent else split_on_3))
        q1, q2 = generated_sent.split(split_on)
        q1s = dedup([parse_enumerated(x) for x in q1.strip().replace("Q1. ", "").replace("Replacements for the subject token", "").split("\n") if len(x) > 15 and " " in x])
        q2s = dedup([parse_enumerated(x) for x in q2.strip().replace("Q2. ", "").replace("Replacements for the object token", "").split("\n") if len(x) > 15 and " " in x])

        q1s = parse_contracted_list_of_spo(q1s)
        q2s = parse_contracted_list_of_spo(q2s)

        return {
                "UnAffectedNeighborhoodMaker_gptinput": generation_query,
                "UnAffectedNeighborhoodMaker_gptoutput": generated_sent,
                "subject_replaced": q1s,
                "object_replaced": q2s
        }


class AffectedReasoningStepsMaker(Prompt):
    def __init__(self, engine: str, openai_wrapper: OpenAIWrapper) -> None:
        super().__init__()
        self.openai_wrapper = openai_wrapper
        self.engine = engine

    def make_query(self, s: str) -> str:
        return f"{s}. Explain this with a 2-step reasoning chain of very short, simple, connected sentences:\n"

    def __call__(self, s: str, subj: str= None, obj: str= None, verb: str= None) -> Dict[str, Any]:
        generation_query = self.make_query(s=s)
        generated_sent = self.openai_wrapper.call(
            prompt=generation_query, engine=self.engine, max_tokens=100, stop_token="###", temperature=0.0)
        split_enumeration_on = "\n" if "\n" in generated_sent.strip() else ". "  # GPT can return a list as a csv or as enumeration
        return {
            "AffectedReasoningStepsMaker_gptinput": generation_query,
            "AffectedReasoningStepsMaker_gptoutput": generated_sent,
            "affected_reasoning": [compress_obj_text(x).split(". ")[-1] for x in generated_sent.split(split_enumeration_on) if x.strip()]
        }


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    supported_inference_types = ["TFGrammatikerPos", "TFGrammatikerNeg",
                                 "AffectedReasoningStepsMaker", "UnAffectedNeighborhoodMaker",
                                 "AffectedNeighborhoodMaker", "AffectedParaphraseMaker",
                                 "TFGrammatikerPosBatched", "TFGrammatikerNegBatched"]

    parser.add_argument("--inference_types", required=True, type=str, help=f"Use one or more (csv) of {','.join(supported_inference_types)}.")
    parser.add_argument("--in_path", required=True, type=str, help="dataset csv json file.")
    parser.add_argument("--out_path", required=True, type=str, help="output stored in this json file.")
    parser.add_argument("--num_samples", default=-1, required=False, type=int, help="Number of samples from in_path to use (default=-1 i.e., all)")
    parser.add_argument("--cache_path", default="memit_csk_dataset/data/cache/cache.jsonl", required=False, type=str, help="GPT3 responses will be cached.")
    parser.add_argument("--gpt3_engine", default="text-davinci-003", required=False, type=str, help="GPT3 model to use.")
    parser.add_argument("--csv_batch_size", default=1, required=False, type=int, help="For multiple inputs to work in one prompt. num csv entries (e.g., 30) to process at a time.")
    args = parser.parse_args()

    openai_wrapper = OpenAIWrapper(cache_path=args.cache_path)

    inferences = []
    if "TFGrammatikerPos" in args.inference_types:
        inferences.append(TFGrammatikerPos(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))
    if "TFGrammatikerPosBatched" in args.inference_types:
        inferences.append(TFGrammatikerPosBatched(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))
    if "TFGrammatikerNeg" in args.inference_types:
        inferences.append(TFGrammatikerNeg(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))
    if "TFGrammatikerNegBatched" in args.inference_types:
        inferences.append(TFGrammatikerNegBatched(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))
    if "AffectedReasoningStepsMaker" in args.inference_types:
        inferences.append(AffectedReasoningStepsMaker(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))
    if "UnAffectedNeighborhoodMaker" in args.inference_types:
        inferences.append(UnAffectedNeighborhoodMaker(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))
    if "AffectedNeighborhoodMaker" in args.inference_types:
        inferences.append(AffectedNeighborhoodMaker(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))
    if "AffectedParaphraseMaker" in args.inference_types:
        inferences.append(AffectedParaphraseMaker(engine=args.gpt3_engine, openai_wrapper=openai_wrapper))

    print(f"\nReading the input from {args.in_path}")
    outputs = []

    arr = utils.read_jsonl_or_json(args.in_path)
    inputs = utils.take(num=args.num_samples, arr=arr)

    print(f"\nDoing inference...")

    if args.csv_batch_size > 1:
        N = args.csv_batch_size
        num_batches = (len(inputs) // N) + (0 if len(inputs)% N ==0 else 1)
        batches = [inputs[i * N: (i * N) + N] for i in range(num_batches)]
        for input_entry in tqdm(batches):
            for inference in inferences:
                assert "Batched" in str(inference.__class__), f"Requested batched output but inference type is {inference.__class__}"
                outputs.append(inference(", ".join(input_entry)))
    else:
        for input_entry in tqdm(inputs):
            local_outputs = []
            for inference in inferences:
                entry = input_entry["prompt"]
                if "prompt_polarity_grounded" in input_entry and "AffectedReasoningStepsMaker" in str(inference.__class__):
                    entry = input_entry.get("prompt_polarity_grounded")
                try:
                    local_outputs.append(inference(s=entry, subj=input_entry["subject"], obj=input_entry["object"], verb=input_entry["verb"]))
                except Exception as exc:
                    print(f"Exception ({exc}) in {entry}")
                    local_outputs.append({})

            # The following is a special case when the ground truth `label=False`, then prompt must be negated
            # in order to get meaningful reasoning steps. Run this as a standalone task as listed in memit/memit-data.sh
            if args.inference_types == "TFGrammatikerNeg":
                # Sample "input_entry" data point.
                #
                # {
                #         "id": 1004,
                #         "prompt": "People do not eat reefer",
                #         "gpt2-xl_predicted_wrong": true,
                #         "gpt2-l_predicted_wrong": true,
                #         "label": " True",
                #         "subject": "People",
                #         "verb": "eat",
                #         "object": "reefer",
                #         "neg": "not"
                #     }
                #
                input_entry["prompt_polarity_grounded"] = local_outputs[0]["negated"] if input_entry["label"].strip().lower() == "false" else input_entry["prompt"]
                outputs.append(input_entry)
            #
            # This is the usual case.
            else:
                outputs.append(
                    {
                        "inputs": input_entry,
                        "outputs": local_outputs
                    }
                )

    print(f"\nWriting the output to {args.out_path}\n\n\n")
    utils.write_jsonl(outpath=args.out_path, data_points=[f for f in outputs if f is not None])
